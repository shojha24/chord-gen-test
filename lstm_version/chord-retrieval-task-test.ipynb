{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10947243,"sourceType":"datasetVersion","datasetId":6809152},{"sourceId":10958818,"sourceType":"datasetVersion","datasetId":6817618}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!apt install ffmpeg\n!pip install \"audio-separator[cpu]\"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:23.472633Z","iopub.execute_input":"2025-03-10T22:58:23.473002Z","iopub.status.idle":"2025-03-10T22:58:30.144343Z","shell.execute_reply.started":"2025-03-10T22:58:23.472976Z","shell.execute_reply":"2025-03-10T22:58:30.142982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Processing a song to extract its notes\n\n# Beat tracking example\nimport librosa\nimport librosa.display as disp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom audio_separator.separator import Separator\n\nfilename = \"/kaggle/input/songstotest/Treat You Better.wav\"\n\n# Initialize the Separator class (with optional configuration properties, below)\nseparator = Separator(output_dir=\"/kaggle/working\")\n\n# Load a machine learning model (if unspecified, defaults to 'model_mel_band_roformer_ep_3005_sdr_11.4360.ckpt')\nseparator.load_model()\n\n# Perform the separation on specific audio files without reloading the model\noutput_files = separator.separate(filename)\n\nprint(f\"Separation complete! Output file(s): {' '.join(output_files)}\")\n\ny, sr = librosa.load(filename)\n\n# 3. Run the default beat tracker\ntempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n\n#print('Estimated tempo: {:.2f} beats per minute'.format,tempo)\n\n# 4. Convert the frame indices of beat events into timestamps\nbeat_times = librosa.frames_to_time(beat_frames, sr=sr)\nprint(beat_times)\n\ny_harmonic, y_percussive = librosa.effects.hpss(y)\n\n# Compute chroma features from the harmonic signal\nchroma = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n\nprint(chroma)\n\nfig, ax = plt.subplots(nrows=1)\nimg = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', ax=ax)\nax.set(title='chroma')\nfig.colorbar(img, ax=ax)\nplt.savefig('test.png')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:48.391076Z","iopub.execute_input":"2025-03-10T22:58:48.391428Z","iopub.status.idle":"2025-03-10T22:58:50.683418Z","shell.execute_reply.started":"2025-03-10T22:58:48.391400Z","shell.execute_reply":"2025-03-10T22:58:50.681829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map chroma indices to note names\nnote_to_midi = {\n    'C': 60, 'C#': 61, 'D': 62, 'D#': 63, 'E': 64, 'F': 65, \n    'F#': 66, 'G': 67, 'G#': 68, 'A': 69, 'A#': 70, 'B': 71\n}\nnote_names = list(note_to_midi.keys())\n\n# Extract the most dominant note at each time frame\n\"\"\"dominant_notes = [\n    [note_to_midi[note_names[idx]] for idx in np.argsort(chroma[:, i])[-3:][::-1]]  # Get top 3 indices, reverse for descending order\n    for i in range(chroma.shape[1])\n]\"\"\"\n\n# apparently the best threshold to capture harmonics while not capturing background noise\nthreshold = 0.75\n\ndominant_notes = [\n    [note_to_midi[note_names[idx]] for idx in range(12) if chroma[idx, i] > threshold]\n    for i in range(chroma.shape[1])\n]\n\n# Get corresponding time values\ntime_values = librosa.frames_to_time(range(chroma.shape[1]), sr=sr)\n\ncurr_time = 0\ncurr_beat = beat_times[0]\ncurr_beat_index = 0\nMAX_BEAT_INDEX = len(beat_times)\n\nnew_time_values = []\nnew_dominant_notes = []\n\nfor i in range(len(time_values)):\n    curr_time = time_values[i]\n    if curr_time > curr_beat:\n        new_time_values.append(time_values[i - 1])\n        new_dominant_notes.append(dominant_notes[i - 1])\n        curr_beat_index+=1\n        if curr_beat_index >= MAX_BEAT_INDEX:\n            break\n        curr_beat = beat_times[curr_beat_index] \n\n# Combine time and note information\nnote_timeline = list(zip(new_time_values, new_dominant_notes))\nprint(new_dominant_notes)\n#print(note_timeline)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:32.361007Z","iopub.status.idle":"2025-03-10T22:58:32.361325Z","shell.execute_reply":"2025-03-10T22:58:32.361204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nchord_encodings = {0: 'A#maj', 1: 'A#min', 2: 'Amaj', 3: 'Amin', 4: 'Bmaj', 5: 'Bmin', 6: 'C#maj', 7: 'C#min', \n                   8: 'Cmaj', 9: 'Cmin', 10: 'D#maj', 11: 'D#min', 12: 'Dmaj', 13: 'Dmin', 14: 'Emaj', 15: 'Emin', \n                   16: 'F#maj', 17: 'F#min', 18: 'Fmaj', 19: 'Fmin', 20: 'G#maj', 21: 'G#min', 22: 'Gmaj', \n                   23: 'Gmin', 24: 'N.C.'}\n\n# Load the model from the H5 file\nmodel = tf.keras.models.load_model('/kaggle/input/test-model/BestChordPredictor.keras')\n\nmax_sequence_length = 16\n\ndef predict_chords(model, note_sequences):\n    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(note_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n    predictions = model.predict(padded_sequences)\n    return np.argmax(predictions, axis=1)  # Return predictions for each sequence\n\npredicted_chords = predict_chords(model, new_dominant_notes)\n\n\n#print(f\"Predicted chord nums: {predicted_chords}\")\nchords = [chord_encodings[chord] for chord in predicted_chords]\nstr_beats = [str(round(beat, 3)) for beat in beat_times]\n\n#print(f\"Predicted chords: {chords}\")\n\nchords_per_beat = dict(zip(str_beats, chords))\n\n# Store keys to delete\nkeys_to_delete = []\n\nfor i in range(1, len(chords)):\n    if chords[i] == chords[i - 1]:\n        keys_to_delete.append(str_beats[i])\n\n# Delete keys after iteration\nfor key in keys_to_delete:\n    chords_per_beat.pop(key, None)  # Using .pop() to avoid KeyError\n\nprint(chords_per_beat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:32.362221Z","iopub.status.idle":"2025-03-10T22:58:32.362519Z","shell.execute_reply":"2025-03-10T22:58:32.362414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pretty_midi\nimport pretty_midi\n\nchord_to_notes = {\n    \"Cmaj\": [60, 64, 67, 72],\n    \"Cmin\": [60, 63, 67, 72],\n    \n    \"C#maj\": [61, 65, 68, 73],\n    \"C#min\": [61, 64, 68, 73],\n    \n    \"Dmaj\": [62, 66, 69, 74],\n    \"Dmin\": [62, 65, 69, 74],\n    \n    \"D#maj\": [63, 67, 70, 75],\n    \"D#min\": [63, 66, 70, 75],\n    \n    \"Emaj\": [64, 68, 71, 76],\n    \"Emin\": [64, 67, 71, 76],\n    \n    \"Fmaj\": [65, 69, 72, 77],\n    \"Fmin\": [65, 68, 72, 77],\n    \n    \"F#maj\": [66, 70, 73, 78],\n    \"F#min\": [66, 69, 73, 78],\n    \n    \"Gmaj\": [67, 71, 74, 79],\n    \"Gmin\": [67, 70, 74, 79],\n    \n    \"G#maj\": [68, 72, 75, 80],\n    \"G#min\": [68, 71, 75, 80],\n    \n    \"Amaj\": [69, 73, 76, 81],\n    \"Amin\": [69, 72, 76, 81],\n    \n    \"A#maj\": [70, 74, 77, 82],\n    \"A#min\": [70, 73, 77, 82],\n    \n    \"Bmaj\": [71, 75, 78, 83],\n    \"Bmin\": [71, 74, 78, 83]\n}\n\n# Create a PrettyMIDI object\nmidi = pretty_midi.PrettyMIDI()\n\n# Create an instrument (piano)\ninstrument = pretty_midi.Instrument(program=0)\n\nsorted_beats = sorted(float(b) for b in chords_per_beat.keys())\n\nfor i in range(len(sorted_beats) - 1):\n    start_time = sorted_beats[i]\n    end_time = sorted_beats[i + 1]\n    chord = chords_per_beat[str(sorted_beats[i])]\n\n    if chord in chord_to_notes:\n        notes = chord_to_notes[chord]\n\n        for note in notes:\n            midi_note = pretty_midi.Note(\n                velocity=80, pitch=note, start=start_time, end=end_time\n            )\n            instrument.notes.append(midi_note)\n\n# Add instrument to MIDI\nmidi.instruments.append(instrument)\n\n# Save MIDI file\nmidi.write(\"chords.mid\")\nprint(\"MIDI file saved as chords.mid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:32.363173Z","iopub.status.idle":"2025-03-10T22:58:32.363467Z","shell.execute_reply":"2025-03-10T22:58:32.363337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install fluidsynth\n!pip install midi2audio\nfrom midi2audio import FluidSynth\n\n# Convert MIDI to WAV using a soundfont\nmidi_file = \"/kaggle/working/chords.mid\"\noutput_wav = \"/kaggle/working/midi_audio.wav\"\nsoundfont = \"/kaggle/input/songstotest/FluidR3_GM.sf2\"  # Use a GM-compatible soundfont (.sf2)\n\nfs = FluidSynth(soundfont)\nfs.midi_to_audio(midi_file, output_wav)\n\nprint(f\"Converted {midi_file} to {output_wav}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:32.364319Z","iopub.status.idle":"2025-03-10T22:58:32.364622Z","shell.execute_reply":"2025-03-10T22:58:32.364494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\nimport re\n\n# Load both WAV files\nmidi_audio = AudioSegment.from_wav(\"/kaggle/working/midi_audio.wav\")\noriginal_audio = AudioSegment.from_wav(filename)\n\nmidi_audio += 2\noriginal_audio -= 16\n\n# Ensure both files have the same duration (trim or loop if needed)\nmin_length = min(len(midi_audio), len(original_audio))\nmidi_audio = midi_audio[:min_length]\noriginal_audio = original_audio[:min_length]\n\n# Mix the two audio files together (adjust volume if necessary)\ncombined_audio = original_audio.overlay(midi_audio, position=0)\n\n# Export the final mix\ncombined_audio.export(f\"final_output.wav\", format=\"wav\")\n\nprint(\"Final mixed WAV file saved as final_output.wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:58:32.365630Z","iopub.status.idle":"2025-03-10T22:58:32.365968Z","shell.execute_reply":"2025-03-10T22:58:32.365838Z"}},"outputs":[],"execution_count":null}]}