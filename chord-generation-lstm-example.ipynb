{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10480441,"sourceType":"datasetVersion","datasetId":6489604},{"sourceId":10768712,"sourceType":"datasetVersion","datasetId":6680282}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharabhojha/chord-generation-lstm-example?scriptVersionId=222966548\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:06:18.360778Z","iopub.execute_input":"2025-02-17T07:06:18.361271Z","iopub.status.idle":"2025-02-17T07:06:18.365274Z","shell.execute_reply.started":"2025-02-17T07:06:18.361231Z","shell.execute_reply":"2025-02-17T07:06:18.364252Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Preprocessing steps:\n# 1: condense onset files to just onset times and onset notes\n\nimport os\nimport re\nimport pandas as pd\n\n# Directory path to your annotations folder\nannotations_dir = \"/kaggle/input/aam-annotations/AAM-annotations/\"\nnum = 0\n\n# Iterate through all files in the directory\nfor filename in os.listdir(annotations_dir):\n    if \"onsets\" in filename and filename.endswith(\".arff\"):  # Ensure it's an ARFF file with 'onsets' in its name\n        num += 1\n        file_path = os.path.join(annotations_dir, filename)\n        \n        # Read the ARFF file\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            lines = file.readlines()\n\n        attributes = []\n        data_start = False\n        data_rows = []\n\n        for line in lines:\n            line = line.strip()\n            \n            # Ignore comments and empty lines\n            if not line or line.startswith(\"%\"):\n                continue\n\n            if line.lower().startswith(\"@attribute\"):\n                # Extract attribute name (between the first space and the last space)\n                match = re.match(r\"@attribute\\s+['\\\"]?([\\w\\s]+)['\\\"]?\\s+.*\", line, re.IGNORECASE)\n                if match:\n                    attributes.append(match.group(1).strip())\n\n            elif line.lower().startswith(\"@data\"):\n                data_start = True  # Data section starts\n            \n            elif data_start:\n                # Split considering quoted strings properly\n                values = re.findall(r'\\\".*?\\\"|\\'.*?\\'|[^,]+', line)\n                values = [v.strip(\"\\\"' \") if v.strip() else None for v in values]  # Remove extra quotes\n                data_rows.append(values)\n\n        # Convert to DataFrame\n        df = pd.DataFrame(data_rows, columns=attributes)\n\n        # Convert numeric columns where possible\n        for col in df.columns:\n            try:\n                df[col] = pd.to_numeric(df[col])  # Convert if possible\n            except ValueError:\n                pass  # Keep as string if conversion fails\n\n        all_onsets = []\n\n        # collect all onset events per timestamp\n        for row in range(df.index.size):\n            notes_at_onset = []\n            for col in range(1, df.columns.size):\n                notes_at_onset.append(df.iat[row, col])\n            notes_at_onset = re.findall(\"(\\d+)\", ''.join(notes_at_onset))\n            all_onsets.append([int(item) for item in notes_at_onset])\n\n        # delete onset events for individual instruments and add column for all events\n        allCols = df.columns[df.apply(lambda col: col.astype(str).str.contains(r\"\\[\", regex=True)).any()].tolist()\n        df.drop(allCols, axis=1, inplace=True)\n        df[\"Onset events\"] = all_onsets\n\n        # Save the processed DataFrame to a new CSV file\n        output_file = re.search(\"(\\d+)\", filename).group(0) + \"_onset_condensed.csv\"\n        df.to_csv(output_file, index=False)\n\n        if num % 100 == 0:\n            print(f\"Processed {filename} and saved to {output_file}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-17T07:06:18.366204Z","iopub.execute_input":"2025-02-17T07:06:18.366549Z","iopub.status.idle":"2025-02-17T07:17:56.473025Z","shell.execute_reply.started":"2025-02-17T07:06:18.366523Z","shell.execute_reply":"2025-02-17T07:17:56.471829Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processed 0521_onsets.arff and saved to 0521_onset_condensed.csv\nProcessed 2699_onsets.arff and saved to 2699_onset_condensed.csv\nProcessed 2634_onsets.arff and saved to 2634_onset_condensed.csv\nProcessed 0544_onsets.arff and saved to 0544_onset_condensed.csv\nProcessed 1658_onsets.arff and saved to 1658_onset_condensed.csv\nProcessed 1624_onsets.arff and saved to 1624_onset_condensed.csv\nProcessed 1192_onsets.arff and saved to 1192_onset_condensed.csv\nProcessed 0001_onsets.arff and saved to 0001_onset_condensed.csv\nProcessed 0382_onsets.arff and saved to 0382_onset_condensed.csv\nProcessed 1000_onsets.arff and saved to 1000_onset_condensed.csv\nProcessed 0541_onsets.arff and saved to 0541_onset_condensed.csv\nProcessed 1630_onsets.arff and saved to 1630_onset_condensed.csv\nProcessed 0696_onsets.arff and saved to 0696_onset_condensed.csv\nProcessed 0121_onsets.arff and saved to 0121_onset_condensed.csv\nProcessed 0097_onsets.arff and saved to 0097_onset_condensed.csv\nProcessed 0220_onsets.arff and saved to 0220_onset_condensed.csv\nProcessed 1161_onsets.arff and saved to 1161_onset_condensed.csv\nProcessed 2537_onsets.arff and saved to 2537_onset_condensed.csv\nProcessed 1108_onsets.arff and saved to 1108_onset_condensed.csv\nProcessed 0558_onsets.arff and saved to 0558_onset_condensed.csv\nProcessed 0564_onsets.arff and saved to 0564_onset_condensed.csv\nProcessed 2885_onsets.arff and saved to 2885_onset_condensed.csv\nProcessed 0646_onsets.arff and saved to 0646_onset_condensed.csv\nProcessed 2182_onsets.arff and saved to 2182_onset_condensed.csv\nProcessed 1541_onsets.arff and saved to 1541_onset_condensed.csv\nProcessed 1601_onsets.arff and saved to 1601_onset_condensed.csv\nProcessed 0352_onsets.arff and saved to 0352_onset_condensed.csv\nProcessed 1210_onsets.arff and saved to 1210_onset_condensed.csv\nProcessed 1189_onsets.arff and saved to 1189_onset_condensed.csv\nProcessed 2995_onsets.arff and saved to 2995_onset_condensed.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 2: encode chord names and replace said chord names with encodings in beatinfo files\n\n# Directory path to your annotations folder\nheaders = ['Start time in seconds', 'Bar count', 'Quarter count', 'Chord name']\n\nchords = set()\n\ndataframes = []\nfilenames = []\n\n# Iterate through all files in the directory\nfor filename in os.listdir(annotations_dir):\n    if \"beatinfo\" in filename and filename.endswith(\".arff\"):  # Ensure it's an ARFF file with 'beatinfo' in its name\n        file_path = os.path.join(annotations_dir, filename)\n        filenames.append(filename)\n        df = pd.read_csv(file_path, comment='@', header=None)\n        df.columns = headers\n\n        for i in range(df.index.size):\n            df.iat[i, 3] = df.iat[i, 3].replace(\"'\", \"\")\n            if df.iat[i, 3] == \"BASS_NOTE_EXCEPTION\":\n                df.iat[i, 3] = \"N.C.\"\n            chords.add(df.iat[i, 3])\n    \n        dataframes.append(df)\n\nsorted_chords = sorted(list(chords))\nchord_encodings = dict(zip([i for i in range(len(chords))], sorted_chords))\nprint(chord_encodings)\n\n# Modify existing dataframes to match encodings\nfor i in range(len(dataframes)):\n    dataframes[i].drop(columns=['Bar count', 'Quarter count'], inplace=True)\n\n    for j in range(dataframes[i].index.size):\n        \n        dataframes[i].iat[j, 1] = sorted_chords.index(dataframes[i].iat[j, 1])\n\n    dataframes[i].to_csv(filenames[i].replace('arff', 'csv'), index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:17:56.474511Z","iopub.execute_input":"2025-02-17T07:17:56.474739Z","iopub.status.idle":"2025-02-17T07:19:25.674357Z","shell.execute_reply.started":"2025-02-17T07:17:56.474719Z","shell.execute_reply":"2025-02-17T07:19:25.672967Z"}},"outputs":[{"name":"stdout","text":"{0: 'A#maj', 1: 'A#min', 2: 'Amaj', 3: 'Amin', 4: 'Bmaj', 5: 'Bmin', 6: 'C#maj', 7: 'C#min', 8: 'Cmaj', 9: 'Cmin', 10: 'D#maj', 11: 'D#min', 12: 'Dmaj', 13: 'Dmin', 14: 'Emaj', 15: 'Emin', 16: 'F#maj', 17: 'F#min', 18: 'Fmaj', 19: 'Fmin', 20: 'G#maj', 21: 'G#min', 22: 'Gmaj', 23: 'Gmin', 24: 'N.C.'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# visualize the files\nworking_dir = \"/kaggle/working/\"\nonsets = pd.read_csv(working_dir + \"0001_onset_condensed.csv\")\nprint(onsets.head())\nbeatinfo = pd.read_csv(working_dir + \"0001_beatinfo.csv\")\nprint(beatinfo.head())\n\ndef align_onsets_with_chords(onsets, beatinfo):\n    aligned_data = []\n    for _, onset_row in onsets.iterrows():\n        onset_time = onset_row['Onset time in seconds']\n        # Find the chord corresponding to this onset time\n        chord_row = beatinfo[beatinfo['Start time in seconds'] <= onset_time].iloc[-1]\n        onset_list = eval(onset_row['Onset events'])\n        aligned_data.append((onset_list, chord_row['Chord name']))\n    return aligned_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:19:25.675837Z","iopub.execute_input":"2025-02-17T07:19:25.676143Z","iopub.status.idle":"2025-02-17T07:19:25.698028Z","shell.execute_reply.started":"2025-02-17T07:19:25.676118Z","shell.execute_reply":"2025-02-17T07:19:25.696981Z"}},"outputs":[{"name":"stdout","text":"   Onset time in seconds  Onset events\n0               0.000000  [41, 60, 65]\n1               0.326086      [41, 60]\n2               0.652173  [41, 65, 65]\n3               0.978259  [41, 65, 69]\n4               1.304346  [41, 65, 65]\n   Start time in seconds  Chord name\n0               0.000000          18\n1               0.652174          18\n2               1.304348          18\n3               1.956522          18\n4               2.608696           0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# create aligned data for every onset and beatinfo file\n\nall_data = []\n\nfor filename in os.listdir(working_dir):\n    if \"onset\" in filename:\n        onset_path = os.path.join(working_dir, filename)\n        beatinfo_path = os.path.join(working_dir, re.search(\"(\\d+)\", filename).group(0) + \"_beatinfo.csv\")\n        onsets = pd.read_csv(onset_path)\n        beatinfo = pd.read_csv(beatinfo_path)\n        all_data += align_onsets_with_chords(onsets, beatinfo)\n\nprint(all_data[0:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:19:25.698869Z","iopub.execute_input":"2025-02-17T07:19:25.699116Z","iopub.status.idle":"2025-02-17T07:28:53.113556Z","shell.execute_reply.started":"2025-02-17T07:19:25.699093Z","shell.execute_reply":"2025-02-17T07:28:53.112705Z"}},"outputs":[{"name":"stdout","text":"[([70, 36, 42, 39, 58, 63, 66, 61], 11.0), ([63, 42, 39, 58, 63, 66, 61], 11.0), ([70, 40, 42, 39, 58, 63, 66, 59], 11.0), ([66, 36, 42, 39, 58, 63, 66], 11.0), ([70, 36, 42, 39, 58, 63, 66, 59], 11.0), ([63, 42, 39, 58, 63, 66, 65], 11.0), ([66, 36, 40, 42, 39, 58, 63, 66, 59], 11.0), ([63, 42, 39, 58, 63, 66, 59], 11.0), ([65, 36, 42, 46, 58, 61, 65, 59], 1.0), ([70, 42, 46, 58, 61, 65], 1.0), ([65, 40, 42, 46, 58, 61, 65, 59], 1.0), ([61, 36, 42, 46, 58, 61, 65], 1.0), ([65, 36, 42, 46, 58, 61, 65, 61], 1.0), ([70, 42, 46, 58, 61, 65, 61], 1.0), ([65, 36, 40, 42, 46, 58, 61, 65, 59], 1.0), ([61, 42, 46, 58, 61, 65], 1.0), ([68, 36, 42, 37, 56, 61, 65, 59], 6.0), ([61, 42, 37, 56, 61, 65, 65], 6.0), ([68, 40, 42, 37, 56, 61, 65, 59], 6.0), ([65, 36, 42, 37, 56, 61, 65], 6.0)]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_chord_classification_model(vocab_size, embedding_dim, lstm_units, num_classes, max_sequence_length):\n    # Input for note sequences\n    note_input = Input(shape=(max_sequence_length,))\n    \n    # Embedding layer for note sequences\n    note_embedding = Embedding(vocab_size, embedding_dim)(note_input)\n    \n    # LSTM layers\n    lstm_output = LSTM(lstm_units, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(note_embedding) # with more lstm layers, return sequences = true\n    lstm_output = BatchNormalization()(lstm_output)\n    #lstm_output = LSTM(lstm_units, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(lstm_output)\n    \n    # Output layer\n    output = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(lstm_output)\n    \n    model = Model(inputs=note_input, outputs=output)\n    return model\n\n# Hyperparameters\nvocab_size = 128  # Assuming MIDI note range\nembedding_dim = 32\nlstm_units = 64\nnum_classes = 25  # Number of chord classes\nmax_sequence_length = 4  # Adjust based on your data\n\n# Create the model\nmodel = create_chord_classification_model(vocab_size, embedding_dim, lstm_units, num_classes, max_sequence_length)\n\n# Compile the model\noptimizer = Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to prepare data\ndef prepare_data(data, max_sequence_length):\n    X = []\n    y = []\n    for sequence in data:\n        notes, chord = sequence\n        padded_notes = tf.keras.preprocessing.sequence.pad_sequences([notes], maxlen=max_sequence_length, padding='post', truncating='post')[0]\n        X.append(padded_notes)\n        y.append(chord)\n    return np.array(X), np.array(y)\n\n# Prepare your data\nX, y = prepare_data(all_data, max_sequence_length)\n\n# Convert y to one-hot encoded format\ny_onehot = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n\n# Train the model\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nhistory = model.fit(X, y_onehot, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping])\n\n# Function for inference\ndef predict_chord(model, note_sequence):\n    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([note_sequence], maxlen=max_sequence_length, padding='post', truncating='post')\n    predictions = model.predict(padded_sequence)\n    return np.argmax(predictions[0])  # Return the prediction\n\n# Example usage\nsample_sequence = [60, 64, 67, 72]  # C major chor\npredicted_chord = predict_chord(model, sample_sequence)\nprint(f\"Predicted chord num: {predicted_chord}\")\nprint(f\"Predicted chord: {chord_encodings[predicted_chord]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:03:43.748118Z","iopub.execute_input":"2025-02-17T08:03:43.748541Z","iopub.status.idle":"2025-02-17T08:14:26.772828Z","shell.execute_reply.started":"2025-02-17T08:03:43.748507Z","shell.execute_reply":"2025-02-17T08:14:26.77089Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m42808/42808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 4ms/step - accuracy: 0.6353 - loss: 1.5936 - val_accuracy: 0.6959 - val_loss: 1.1297\nEpoch 2/100\n\u001b[1m42808/42808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 4ms/step - accuracy: 0.6884 - loss: 1.1828 - val_accuracy: 0.7060 - val_loss: 1.0550\nEpoch 3/100\n\u001b[1m42808/42808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 4ms/step - accuracy: 0.6984 - loss: 1.1077 - val_accuracy: 0.7113 - val_loss: 1.0205\nEpoch 4/100\n\u001b[1m15103/42808\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 4ms/step - accuracy: 0.7037 - loss: 1.0659","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-146c3cd07bab>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Function for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_pure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     )\n\u001b[0;32m--> 862\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36margs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_VAR_POSITIONAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2802\u001b[0m                     \u001b[0;31m# *args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mkind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2699\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11}]}